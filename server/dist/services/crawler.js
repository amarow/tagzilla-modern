"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.crawlerService = void 0;
const repository_1 = require("../db/repository");
const fs_1 = __importDefault(require("fs"));
const path_1 = __importDefault(require("path"));
const worker_threads_1 = require("worker_threads");
class CrawlerWorkerPool {
    constructor() {
        this.workers = [];
        this.taskQueue = [];
        this.activeWorkers = 0;
        this.maxWorkers = 4; // User requested 8 cores
        // Determine correct path and execution args based on environment (ts-node vs node)
        const isTsNode = __filename.endsWith('.ts');
        this.workerScript = isTsNode
            ? path_1.default.join(__dirname, 'crawler.worker.ts')
            : path_1.default.join(__dirname, 'crawler.worker.js');
    }
    getWorkerOptions() {
        const isTsNode = __filename.endsWith('.ts');
        return isTsNode ? {
            execArgv: ['-r', 'ts-node/register']
        } : {};
    }
    processFile(filePath, extension, fileId) {
        return new Promise((resolve, reject) => {
            const task = { filePath, extension, fileId, resolve, reject };
            this.scheduleTask(task);
        });
    }
    scheduleTask(task) {
        if (this.activeWorkers < this.maxWorkers) {
            this.runTask(task);
        }
        else {
            this.taskQueue.push(task);
        }
    }
    runTask(task) {
        this.activeWorkers++;
        // Lazy initialization of workers or reuse?
        // For simplicity and robustness, let's spawn a new worker or grab from a pool.
        // Actually, creating a Worker per file is expensive. We should keep them alive.
        // Let's implement a simple persistent pool.
        let worker;
        if (this.workers.length < this.maxWorkers) {
            worker = new worker_threads_1.Worker(this.workerScript, this.getWorkerOptions());
            this.workers.push(worker);
            // Setup generic listener for this worker to handle pool logic?
            // No, standard pool pattern: Assign task to specific worker.
            // But 'Worker' is an actor.
            // Refined approach:
            // 1. We have N workers.
            // 2. We send a message to a free worker.
            // 3. When worker replies, we mark it free and assign next task.
        }
        else {
            // This branch shouldn't be reached if we check activeWorkers correctly vs pool size
            // But if we reuse workers, we need to find an idle one.
            // Simplified: Just use activeWorkers count to throttle, but we need reference to the *idle* worker.
        }
        // Actually, with the "Request/Response" pattern on a single channel per worker:
        // Let's rewrite slightly to manage the *Worker Instances* explicitly.
    }
}
// Simplified Pool for specific use case
const MAX_WORKERS = 8;
const workerPool = [];
const taskQueue = [];
const idleWorkers = [];
function initPool() {
    const isTsNode = __filename.endsWith('.ts');
    const workerScript = isTsNode
        ? path_1.default.join(__dirname, 'crawler.worker.ts')
        : path_1.default.join(__dirname, 'crawler.worker.js');
    const options = isTsNode ? { execArgv: ['-r', 'ts-node/register'] } : {};
    for (let i = 0; i < MAX_WORKERS; i++) {
        const worker = new worker_threads_1.Worker(workerScript, options);
        worker.on('message', (message) => {
            // Find the task that corresponds to this message? 
            // Or simpler: The worker processes one thing at a time. 
            // We need to map the worker back to the promise.
            // We can store a 'currentTask' on the worker object (casted to any).
            const currentTask = worker.currentTask;
            if (currentTask) {
                if (message.status === 'success') {
                    currentTask.resolve({ text: message.text });
                }
                else {
                    currentTask.resolve({ text: null }); // Resolve null on error to keep going
                }
                worker.currentTask = null;
            }
            // Worker is done, check queue
            if (taskQueue.length > 0) {
                const nextTask = taskQueue.shift();
                assignTaskToWorker(worker, nextTask);
            }
            else {
                idleWorkers.push(worker);
            }
        });
        worker.on('error', (err) => {
            console.error('[WORKER ERROR]', err);
            // If a worker dies, we should probably replace it, but for now just log.
            const currentTask = worker.currentTask;
            if (currentTask)
                currentTask.resolve({ text: null });
        });
        workerPool.push(worker);
        idleWorkers.push(worker);
    }
}
function assignTaskToWorker(worker, task) {
    worker.currentTask = task;
    worker.postMessage({ filePath: task.filePath, extension: task.extension, fileId: task.fileId });
}
function processFileWithWorker(filePath, extension, fileId) {
    return new Promise((resolve, reject) => {
        const task = { filePath, extension, fileId, resolve, reject };
        if (idleWorkers.length > 0) {
            const worker = idleWorkers.pop();
            assignTaskToWorker(worker, task);
        }
        else {
            taskQueue.push(task);
        }
    });
}
// Initialize pool on module load or first use
// We'll call it in init()
let poolInitialized = false;
// --- End Worker Pool ---
exports.crawlerService = {
    async init() {
        console.log('[CRAWLER] Initializing...');
        if (!poolInitialized) {
            initPool();
            poolInitialized = true;
            console.log(`[CRAWLER] Worker pool initialized with ${MAX_WORKERS} threads.`);
        }
        const scopes = await repository_1.scopeRepository.getAll();
        for (const s of scopes) {
            const scope = s;
            console.log(`[CRAWLER] Triggering startup scan for scope ${scope.id} (${scope.path})`);
            this.scanScope(scope.id, scope.path).catch(err => {
                console.error(`[CRAWLER] Error scanning scope ${scope.id} on startup:`, err);
            });
        }
    },
    async addScope(userId, directoryPath) {
        if (!fs_1.default.existsSync(directoryPath)) {
            throw new Error(`Directory not found: ${directoryPath}`);
        }
        const scope = await repository_1.scopeRepository.create(userId, directoryPath);
        this.scanScope(scope.id, directoryPath);
        return scope;
    },
    async scanScope(scopeId, directoryPath) {
        console.log(`[CRAWLER] Starting scan for scope ${scopeId}: ${directoryPath}`);
        const scopeObj = await repository_1.scopeRepository.getById(scopeId);
        if (!scopeObj) {
            console.error(`[CRAWLER] Scope ${scopeId} not found`);
            return;
        }
        const appState = await repository_1.appStateRepository.get(scopeObj.userId);
        const searchSettings = appState && appState.search_settings ? appState.search_settings : {};
        // Expanded default allowed extensions
        const allowedExtensions = searchSettings.allowedExtensions ||
            ['.txt', '.md', '.markdown', '.json', '.ts', '.js', '.jsx', '.tsx', '.html', '.css', '.scss', '.xml', '.yaml', '.yml', '.sql', '.env', '.pdf', '.docx'];
        let fileCount = 0;
        let ignoredCount = 0;
        let skippedCount = 0;
        const startTime = Date.now();
        const queue = [{ path: directoryPath, depth: 0 }];
        // We keep track of pending worker tasks to know when the scan is fully complete
        let pendingWorkerTasks = 0;
        const processQueue = async () => {
            // If file system queue is empty
            if (queue.length === 0) {
                // Check if we are waiting for workers
                if (pendingWorkerTasks > 0) {
                    // Check again in a bit
                    setTimeout(processQueue, 100);
                    return;
                }
                const duration = ((Date.now() - startTime) / 1000).toFixed(2);
                console.log(`[CRAWLER] Finished scan for scope ${scopeId}. Total: ${fileCount} files. Skipped (unchanged): ${skippedCount}. Ignored: ${ignoredCount} items. Time: ${duration}s.`);
                return;
            }
            const current = queue.shift();
            try {
                const entries = await fs_1.default.promises.readdir(current.path, { withFileTypes: true });
                for (const entry of entries) {
                    const fullPath = path_1.default.join(current.path, entry.name);
                    if (entry.name.startsWith('.') ||
                        ['node_modules', 'dist', 'build', 'coverage', 'target', 'venv', '__pycache__'].includes(entry.name)) {
                        ignoredCount++;
                        continue;
                    }
                    if (entry.isDirectory()) {
                        if (current.depth < 20) {
                            queue.push({ path: fullPath, depth: current.depth + 1 });
                        }
                        else {
                            console.log(`[CRAWLER] Max depth reached at ${fullPath}`);
                            ignoredCount++;
                        }
                    }
                    else if (entry.isFile()) {
                        try {
                            const stats = await fs_1.default.promises.stat(fullPath);
                            const mtimeStr = stats.mtime.toISOString();
                            // Optimization: Check if file already exists with same mtime and size
                            const existing = repository_1.fileRepository.getFileMinimal(scopeId, fullPath);
                            if (existing && existing.updatedAt === mtimeStr && existing.size === stats.size) {
                                skippedCount++;
                                fileCount++; // Still count as "processed" in total
                            }
                            else {
                                const fileId = await repository_1.fileRepository.upsertFile(scopeId, fullPath, {
                                    size: stats.size,
                                    ctime: stats.ctime,
                                    mtime: stats.mtime
                                });
                                const ext = path_1.default.extname(fullPath).toLowerCase();
                                // Limit: 20MB for binary, 5MB for text
                                const maxSize = (ext === '.pdf' || ext === '.docx') ? 20 * 1024 * 1024 : 5 * 1024 * 1024;
                                if (allowedExtensions.includes(ext) && stats.size < maxSize) {
                                    pendingWorkerTasks++;
                                    // Offload to worker
                                    processFileWithWorker(fullPath, ext, fileId).then(async (result) => {
                                        if (result.text) {
                                            try {
                                                await repository_1.searchRepository.indexContent(fileId, result.text);
                                            }
                                            catch (err) {
                                                console.error(`[CRAWLER] Failed to index content for ${fullPath}:`, err);
                                            }
                                        }
                                    }).catch(err => {
                                        console.error(`[CRAWLER] Worker error for ${fullPath}:`, err);
                                    }).finally(() => {
                                        pendingWorkerTasks--;
                                    });
                                }
                                fileCount++;
                            }
                            if (fileCount % 1000 === 0) {
                                console.log(`[CRAWLER] Scope ${scopeId}: Found ${fileCount} files (Skipped ${skippedCount})...`);
                            }
                        }
                        catch (err) {
                            // ignore individual file errors
                        }
                    }
                }
            }
            catch (err) {
                console.error(`[CRAWLER] Error scanning ${current.path}:`, err);
            }
            // Throttle file system scanning slightly to allow workers to catch up if queue is huge?
            // Actually, with setImmediate, we yield enough.
            setImmediate(processQueue);
        };
        processQueue();
    }
};
